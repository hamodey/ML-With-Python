{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Classification and Cross-Validation\n",
    "\n",
    "**Instructions:**\n",
    "* go through the notebook and complete the **tasks** .  \n",
    "* Make sure you understand the examples given!\n",
    "* When a question allows a free-form answer (e.g., ``what do you observe?``) create a new markdown cell below and answer the question in the notebook.\n",
    "* ** Save your notebooks when you are done! **\n",
    "\n",
    "In the previous lab, we loaded up the iris dataset for flower classification, and performed simple exploratory data analysis, i.e., we visualized the data available (features given class labels) in order to understand characteristics of the data (e.g., that some classes are easier to be separated from others based on some features, etc.)\n",
    "\n",
    "If you don't remember much about this, please revisit the corresponding lab (Lab 2) before moving on.\n",
    "\n",
    "In this lab, we will go through the process of actually training a classifier on a dataset (training set), and evaluating the performance of the classifier on unknown data (test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Task:**</span> Run the cell below to load our data. Notice the last line, where we add some random Gaussian noise to our data to make the task more challenging (data in real life usually contains some form of noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "#view a description of the dataset (uncomment next line to do so)\n",
    "#print(iris.DESCR)\n",
    "\n",
    "#Set X a samples times features matrix, Y equal to the targets\n",
    "X=iris.data \n",
    "y=iris.target \n",
    "\n",
    "\n",
    "#we add some random noise to our data to make the task more challenging\n",
    "X=X+np.random.normal(0,0.4,X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Task:**</span> How many data samples do we have?  Print the value below using ``shape`` on X appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "print(X.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Task:**</span> How many features do we have?  Print the value below using ``shape`` on X appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Task:**</span> How many classes do we have?  Print the value below using ``np.unique`` appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Task:**</span> How many samples do we have that belong to class 1?  Use the ``np.where`` function appropriately on y to print this in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66,\n",
       "        67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83,\n",
       "        84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(y == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Task:**</span> Assume we want to generate a list of shuffled indices of our data.  Use the function ``numpy.random.permute`` to do that.  In the cell below, you can already see how to create a list of indices that is **not** shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 82, 124,  30, 128,  23,  97,  20,  67,  87,  36,  98,  48, 133,\n",
       "        12,  53,  71,   6, 125, 122, 110, 141,  65,  59,  89,  38,  47,\n",
       "        62, 114,  50, 102,  14, 144,  15,  70,  33,  66,  18, 123, 107,\n",
       "        92,  96,  26,  43,   3, 111,  79,  44,  81,  21,  84, 120,  41,\n",
       "       140, 117, 119,  49,   9,  56,   0, 118,  99,  39,  69, 127,  52,\n",
       "        25,  83,  34,  35, 112, 105,  88,  94, 137,  54, 108, 104,   7,\n",
       "       139,  85,  51,  60, 126,   2, 149,  11,  32,  61,  78,  10,  63,\n",
       "        27, 147,  28, 116, 145,   5,  24,  37,  55, 106,  90, 136, 100,\n",
       "        57,  64, 121, 142,  17,  68, 148, 134,  95,  16,  45,  72,  22,\n",
       "       103, 138, 129, 143, 146,  31, 113,  73,  46, 115, 130,  40,   8,\n",
       "        13,  74,  75,  76,  42,  19,  77,   1,  91, 101,  86,  29,  93,\n",
       "        80, 132, 109, 135,  58, 131,   4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L=list(range(X.shape[0]))\n",
    "print(L)\n",
    "#Enter code here\n",
    "\n",
    "np.random.permutation(L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Task:**</span> Here is an example of using the k-NN classifier.  We split our data to training and testing (with a 0.2 percentage for our test data), fit on the training data, test on the testing data.  Go through the code and make sure you understand it.  Subsequently, do the same for the next cell, that prints the confusion matrix and the total accuracy.  (documentation: http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "\n",
    "**note: for this lab, we use the euclidean distance along with 10 neighbours**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 2 2 0 2 1 2 1 0 0 1 1 2 1 1 2 0 1 1 1 0 2 0 2 0 1 0]\n",
      "[1 0 2 1 2 1 0 2 1 2 1 0 0 1 2 2 1 1 2 0 1 2 2 0 2 0 2 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#split to train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "#define knn classifier, with 5 neighbors and use the euclidian distance\n",
    "knn=KNeighborsClassifier(n_neighbors=10, metric='euclidean')\n",
    "#define training and testing data, fit the classifier\n",
    "knn.fit(X_train,y_train)\n",
    "#predict values for test data based on training data\n",
    "y_pred=knn.predict(X_test)\n",
    "#print values\n",
    "print(y_test) # true values\n",
    "print(y_pred) # predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9 0 0]\n",
      " [0 9 3]\n",
      " [0 1 8]]\n",
      "overall accuracy: 0.8666666666666667\n",
      "class precision: [1.         0.9        0.72727273]\n",
      "class recall: [1.         0.75       0.88888889]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('overall accuracy: %s' % accuracy_score(y_test,y_pred))\n",
    "\n",
    "# we can also generate the class-relative precision and recall \n",
    "print('class precision: %s' % precision_score(y_test,y_pred,average=None))\n",
    "print('class recall: %s' %recall_score(y_test,y_pred,average=None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Task:**</span> Write your **own** functions that return the confusion matrix given the true and predicted labels, as well as the accuracy.  To do so, fill in the code in the next two cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 2 2 0 2 1 2 1 0 0 1 1 2 1 1 2 0 1 1 1 0 2 0 2 0 1 0]\n",
      "[1 0 2 1 2 1 0 2 1 2 1 0 0 1 2 2 1 1 2 0 1 2 2 0 2 0 2 0 1 0]\n",
      "arr [0 1 2]\n",
      "[[9 0 0]\n",
      " [0 9 3]\n",
      " [0 1 8]]\n"
     ]
    }
   ],
   "source": [
    "#create a matrix with entries equal to zero, and subsequently build the confusion matrix\n",
    "#the method should return the confusion matrix in a numpy array\n",
    "def myConfMat(y_test,y_pred,classno):\n",
    "    print(y_test)\n",
    "    print(y_pred)\n",
    "\n",
    "    C= np.zeros((classno, classno), dtype=int)\n",
    "    arr = np.unique(y_test)\n",
    "\n",
    "    for i in range(len(y_test)):\n",
    "        if y_pred[i] == y_test[i]:\n",
    "            C[y_test[i], y_pred[i]] += 1\n",
    "        \n",
    "        elif y_pred[i] != y_test[i]:\n",
    "            if y_pred[i] in arr:\n",
    "                C[y_test[i], y_pred[i]] += 1\n",
    "    \n",
    "    print(\"arr\", arr)  \n",
    "    \n",
    "    # initialize the confusion matrix to zeros\n",
    "    #loop through all results and update the confusion matrix\n",
    "    return C\n",
    "\n",
    "#note: len(np.unique(y))  indicates the dimensions of the confusion matrix (why?)\n",
    "print(myConfMat(y_test,y_pred,len(np.unique(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 2 2 0 2 1 2 1 0 0 1 1 2 1 1 2 0 1 1 1 0 2 0 2 0 1 0]\n",
      "[1 0 2 1 2 1 0 2 1 2 1 0 0 1 2 2 1 1 2 0 1 2 2 0 2 0 2 0 1 0]\n",
      "arr [0 1 2]\n",
      "84.61538461538461\n"
     ]
    }
   ],
   "source": [
    "#use the numpy function where to return the accuracy given the true/predicted labels.  i.e., #correct/#total\n",
    "def myAccuracy(y_test,y_pred):\n",
    "    C = myConfMat(y_test,y_pred,len(np.unique(y)))\n",
    "    diagnol = 0\n",
    "    rest = 0\n",
    "    for i in range(len(C)):\n",
    "        for j in range(len(C)):\n",
    "            if i == j:\n",
    "                #print(\"d = \", C[i][j], \" + \")\n",
    "                diagnol += C[i][j]\n",
    "            else:\n",
    "                #print(\"rest = \", C[i][j], \" + \")\n",
    "                rest += C[i][j]\n",
    "    \n",
    "    #print(\"Sum = \", diagnol)\n",
    "    #print(\"Rest = \", rest)\n",
    "    \n",
    "    accuracy=(rest / diagnol) * 100 # change this line\n",
    "    return 100 - accuracy \n",
    "    \n",
    "    \n",
    "print(myAccuracy(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Task:**</span> Write your own cross-validation function.  In this case, we are using a fixed distance (euclidean) and a fixed number of neighbours (10) so we do **not** need to create a validation set.\n",
    "\n",
    "Your function (see cell below) firstly splits the indices of each of our data into bins according to the number of folds (here: 5-fold).\n",
    "\n",
    "Then, you should loop through all folds, split the data into training and testing by selecting the appropriate bins (see slides on cross-validation), train on training data and save the test result as the accuracy for each fold (see list accuracy_fold).  This is the list that your function should return in the end.  Remember that the ``extend`` function extends a list with more values.  \n",
    "\n",
    "The final print call in the end of the cell should print the list of accuracies, with five values, one for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myCrossVal(X,y,foldK):\n",
    "    accuracy_fold=[] #list to store accuracies folds\n",
    "    \n",
    "    \n",
    "    #TASK: use the function np.random.permutation to generate a list of shuffled indices from in the range (0,number of data)\n",
    "    #(you did this already in a task above)\n",
    "    #indices=#...\n",
    "    #print(indices)\n",
    "    \n",
    "    #TASK: use the function array_split to split the indices to k different bins:\n",
    "    #uncomment line below\n",
    "    #bins=\n",
    "    #print(bings)\n",
    "    \n",
    "    \n",
    "    #loop through folds\n",
    "    for i in range(0,foldK):\n",
    "        foldTrain=[] # list to save current indices for training\n",
    "        foldTest=[]  # list to save current indices for testing\n",
    "        #TASK: take bin i for testing, rest for training.  Can use the function extend to add indices to foldTrain and foldTest\n",
    "        #train kNN classifier\n",
    "        #test on test data\n",
    "        #append the new accuracy to your accuracy_fold list.  You can use accuracy_score or your myAccuracy function.\n",
    "    return accuracy_fold;\n",
    "    \n",
    "accuracy_fold=myCrossVal(X,y,5)\n",
    "print(accuracy_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Task:**</span> Print the average accuracy and standard deviation of your results over the 5 folds. (functions ``mean`` and ``std``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Optional task:**</span> Write your own functions to calculate class-relative precision and recall. Compare these to the sklearn functions ``precision_score`` and ``recall_score`` that were used above on the original y_test and y_pred values (from the beginning of this tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hint: you can use the output from your myConfMat function above\n",
    "\n",
    "def myPrecision(y_test,y_pred):\n",
    "    classes = np.unique(y_pred)\n",
    "    precision = np.zeros(nClasses.shape) \n",
    "    return precision\n",
    "\n",
    "def myRecall(y_test,y_pred):\n",
    "    classes = np.unique(y_pred)\n",
    "    recall = np.zeros(nClasses.shape) \n",
    "    return recall\n",
    "\n",
    "print('classes:      %s' % np.unique(y_pred) )    \n",
    "print('my precision: %s' % myPrecision(y_test,y_pred))\n",
    "print('my recall:    %s' % myRecall(y_test,y_pred))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
